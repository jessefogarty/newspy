#!/usr/bin/env python3
import feedparser
from source import Source
from termcolor import cprint
import requests
import concurrent.futures
from newspaper import Article as nArticle

class Articles():
    '''
    '''
    
    def __init__(self, f:list) -> object:
        '''Finds new articles from Source.feeds object. \n
        Args: \n
            f(list) - a list object containing (str) feeds.
        Contains: \n
            article_list(list) - list of unique articles urls from f.
            articles(list)(dicr) - a list of individual article dicts containing parsed data.
        '''
        self.feeds = f
        self.article_list = self.retrieve()
        self.articles = self.download()
        '''with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
            futures = []
            for _url in self.article_list:
                futures.append(executor.submit(self.download, url=_url))
            for future in concurrent.futures.as_completed(futures):
                try:
                    self.articles.append(future.result())'''


    def __repr__(self):
        rep = f"""Article object contains the following:
        # of feeds = {len(self.feeds)}
        # of unique articles: {len(self.article_list)}
        # of downloaded articles: {len(self.articles)}"""
        return rep
        
    def download(self) -> list:
        '''Download the HTML of an article URL. \n
        Args:
            self.article_list - A list of new article URLs generated by Articles.retrieve() \n
        Returns:
            articles(list) - a list of articles with a dict of each article
        '''
        def _download(url) -> object:
            ''' A function to download and parse the html of a given article.
                Uses the Aricle class from newspaper3k (a great project)
            '''
            article = nArticle(url)
            try:
                article.download()
                article.parse()
                article.nlp()
                return article
            except:
                article = None
                return article 

        _articles = []
        _download_errors = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
            futures = []
            for _url in self.article_list:
                if _url is not None:
                    futures.append(executor.submit(_download, url=_url))
                else:
                    _download_errors.append(_url)
            for future in concurrent.futures.as_completed(futures):
                _articles.append(future.result())

        return _articles

        
    def retrieve(self) -> list:
        '''Retrieve each source feed and parse article urls. Remove duplicates.\n
        Returns:
            _articles - a list of unique new article urls\n
        TODO: inlcude a list of existing urls from the same source to compare agaisnt.
        '''
        _articles = []
        #cprint("Finding new _articles and adding to source.", "yellow")
        for i in range(len(self.feeds)):
            if i == 0:
                data = feedparser.parse(self.feeds[i])
                entries = data.entries
                links = [l["link"] for l in entries]
                _articles.extend(links)
            else:
                data = feedparser.parse(self.feeds[i])
                entries = data.entries
                links = [l["link"] for l in entries]
                _articles.extend(l for l in links if l not in _articles)
        #cprint(f"Added {len(_articles)} to source.", "green")
        return _articles